{
  "dataset_reader": {
    "type": "bert_reader",
    "max_seq_length": 64,
    "dupe_factor": 2,
    "short_seq_prob": 0.1,
    "masked_lm_prob": 0.15,
    "max_predictions_per_seq": 3,
    "lazy": false,
    "tokenizer": {
      "type": "jieba",
      "word_splitter": {
        "type": "jieba",
        "pos_tags": true,
      },
      "end_tokens": []
    },
    "token_indexers": {
      "tokens": {
        "type": "single_id",
        "lowercase_tokens": true
      }
    }
  },
  "train_data_path": "/home/icepine.hans/data/bert_shenma_data.train.small.txt",
  "validation_data_path": "/home/icepine.hans/data/bert_shenma_data.validate.txt",
  "model": {
    "type": "bert",
    "use_fp16": false,
    "text_field_embedder": {
      "type": "basic_v2",
      "use_fp16": false,
      "tokens": {
        "type": "embedding_v2",
        "pretrained_file": "/home/icepine.hans/data/ChineseEmbedding.tar.gz",
        "padding_index": 0,
        "embedding_dim": 200,
        "trainable": true
      }
    },
    "transformer": {
      "use_fp16": false,
      "type": "transformer",
      "num_hidden_layers": 16,
      "intermediate_size": 3072,
      "intermediate_act_fn": "gelu",
      "num_heads": 16,
      "input_size": 200,
      "memory_size": 200,
      "key_depth": 1024,
      "value_depth": 1024,
      "max_position_embeddings": 256,
      "type_vocab_size": 2,
      "attention_dropout_prob": 0.1,
      "dropout_prob": 0.1,
      "attention_type": "dot_product"
    },
    "initializer": [
    ]
  },
  "iterator": {
    "type": "bucket",
    "sorting_keys": [
      [
        "tokens",
        "num_tokens"
      ]
    ],
    "batch_size": 256
  },
  "trainer": {
    "num_epochs": 40,
    "patience": 5,
    "cuda_device": [0, 1],
    "grad_clipping": 0.1,
    "validation_metric": "-loss",
    "optimizer": {
      "type": "bert_adam",
      "lr": 0.00001,
    }
  }
}
